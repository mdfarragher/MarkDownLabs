# The MNIST Handwriting Dataset

Optical Character Recognition (OCR) systems are machine learning models that are trained to recognize written text. These systems have many real-world applications, for example in scanning books, printed documents and receipts, processing bank checks and forms, reading car license plates and much more. 

Processing handwriting is an expecially hard challenge to solve, because the letters and numbers are not always the same size and the writing style tends to differ from person to person. In this field, the MNIST dataset is famous. Since its release in 1999, this classic dataset of handwritten digits has served as the basis for benchmarking OCR systems. 

![MNIST Dataset](../img/data.jpg)
{ .img-fluid .pb-4 }

The dataset was created in 1999 by mixing handwriting samples from American Census Bureau employees and American high school students. The black and white images of handwritten digits were normalized to fit into a 28x28 pixel bounding box and anti-aliased to introduce grayscale levels.

In this assignment, you are going to build a C# app that trains a machine learning model to recognize the handwritten digits in the MNIST dataset.

# Get The Data

Let's start by downloading the MNIST dataset. 

{{< encrypt >}}

Grab the file from here: [MNIST dataset](https://csvbase.com/mdfarragher/mnist-handwriting).

Download the file and save it as **Mnist-Handwriting.csv**.

There are 10,000 rows in this dataset. Each row represents one image in monochrome and resized to 28x28 pixels. The dataset is a comma-separated text file with 786 columns:

- The first column is the row identifier, generated by CsvBase.

- The second column contains the label. It tells us which one of the 10 possible digits is visible in the image.

- The remaining 784 columns are the pixel intensity values (0 .. 255) for each pixel in the image, counting from left to right and top to bottom.

You are going to build a multiclass classification machine learning model that reads in all 785 columns, and then makes a prediction for each digit in the dataset.

Let’s get started. You need to build a new application from scratch by opening a terminal and creating a new console project:

#### Set Up The Project

Now open your terminal and navigate to the folder where you want to create the project (e.g., **~/Documents**), and run:

```bash
dotnet new console -o Mnist
cd Mnist
```

This creates a new C# console application with:

- **Program.cs** – your main program file
- **Mnist.csproj** – your project file

Then move the Mnist-Handwriting.csv file into this folder.

Now run the following command to install the Microsoft.ML machine learning library:

```bash
dotnet add package Microsoft.ML
```

Next, we're going to analyze the dataset and come up with a feature engineering plan.

{{< /encrypt >}}

# Analyze The Data

We’ll begin by analyzing the MNIST dataset and come up with a plan for feature engineering. Our goal is to map out all required data transformation steps in advance to make later machine learning training as successful as possible.

{{< encrypt >}}

#### Manually Explore the Data

Let’s start by exploring the dataset manually.

Open **Mnist-Handwriting.csv** in Visual Studio Code, and start looking for patterns, issues, and feature characteristics.

What to look out for:

-    Are there any missing values or inconsistent rows?
-    Are the pixel values in each column all between 0 and 255?
-    Do we have balanced populations of labels?

Write down 3 insights from your analysis.
{.homework}

#### Ask Copilot To Analyze The Dataset

Now expand the Copilot panel in Visual Studio Code and enter the following prompt:

"You are a machine learning expert. Analyze this CSV file for use in a classification model that predicts Label. What problems might the dataset have? What preprocessing steps would you suggest?"
{.prompt}

You can either paste in the column names and 5–10 sample rows, or upload the CSV file directly (if your agent supports file uploads).

![Analyze a dataset with an AI agent](../img/analyze.jpg)
{.img-fluid .mb-4}

#### What Might The Agent Suggest?

The agent may recommend steps like:

-    Normalize pixel columns to a range of 0 .. 1
-    Check for class imbalance and fix with over- or undersampling
-    Remove pixels that are always zero
-    Augment the dataset by translating, rotating and scaling the images

Write down 3 insights from the agent’s analysis.
{.homework}

Next, we'll generate a quick histogram to check if the populations for all classes are balanced. 

{{< /encrypt >}}

# Check For Class Imbalance

In the previous lab module, you checked for class imbalance in the **Sex** column and when you discovered that the Cleveland CAD dataset had many more male patients than female, you undersampled the men to restore the balance. 

Checking for class imbalance is even more important with multiclass classification datasets, because now we have many distinct label classes to predict, and we want to make sure we have the same number of records for each class. This will ensure that the fully trained model is unbiased.

{{< encrypt >}}

So in this section, you’ll going to generate code to plot a histogram for the Label column, to check how often each digit appears in the dataset.

Let's get started.

#### Install Utility Classes And Dependencies

Let's see if Copilot can do the whole thing for us. We'll ask the agent to download the histogram utils class from the Git repository and install any required dependencies for us. 

You'll need the raw url of the utility file hosted in the repository where you've stored the code you created in the previous lab. We'll copy the url into the prompt and ask the agent to import the class directly into the current project.

I pushed my utility class to a repository on Codeberg. You can use my repository url in your prompt if you like:

"Copy the HistogramUtils class from this repository url and add it to the project. Install any required NuGet package dependencies to ensure that the code works: <br> - codeberg.org/mdft/ml-mlnet-csharp/raw/branch/main/HeartDisease/HistogramUtils.cs"
{ .prompt } 

#### Load The Mnist-Handwriting.csv File

Now let's ask Copilot to write the code for loading the CSV file and generating a data class that represents one record from the file. 

Enter the following prompt:

"Write C# code to load the Mnist-Handwriting.csv file, using the LoadFromTextFile method in ML.NET. Also create a data class that represents one record from the file, that can be used with the CreateEnumerable method in ML.NET to create a list of image data records."
{ .prompt }

And let Copilot write the code for you.

You should see the following data loading code in your project:

```csharp
// Create ML.NET context
var mlContext = new MLContext();

// Load data from CSV file using LoadFromTextFile
var dataView = mlContext.Data.LoadFromTextFile<MnistData>(
    path: "Mnist-Handwriting.csv",
    hasHeader: true,
    separatorChar: ',');

// Create an enumerable list of MnistData records
var dataList = mlContext.Data
    .CreateEnumerable<MnistData>(dataView, reuseRowObject: false).ToList();
```

The `MnistData` class looks super interesting:

```csharp
// Data class representing one MNIST handwriting record
public class MnistData
{
    [LoadColumn(0)]
    public float RowID { get; set; }

    [LoadColumn(1)]
    public float Label { get; set; }

    // Load all 784 pixel values (28x28 image)
    [LoadColumn(2, 785)]
    [VectorType(784)]
    public float[] PixelValues { get; set; } = new float[784];
}
```

Note the `VectorType` attribute on the `PixelValues` property. Combined with the `LoadColumn` attribute, it specifies that columns 2 .. 785 of the datafile will be loaded into a single `float[]` aray with 784 elements. 

You can combine multiple columns into float arrays in ML.NET pipelines, and this is very convenient when we're working with image data. A 28x28 pixel image has 784 individual pixels, and you don't want to be tracking the column name of each of them individually. 

#### Generate The Histogram Of Labels

To generate the histogram of labels, we can simply do this:

```csharp
// generate the histogram of labels
Console.WriteLine("Generating histograms of labels...");
var hist = HistogramUtils.PlotHistogram<MnistData>(dataList, "Label");

// save the histogram
hist.SavePng("histogram-labels.png", 800, 600);
```

That histogram utility class comes in really handy every time. 

Homework: add code to generate the histogram of labels. Then run your app and examine the plot. What can you say about any class imbalance in this dataset? Write down your conclusions.  
{ .homework }

Here's what I got:

![Histogram Grid For Full Dataset](../img/histogram-labels.png)
{.img-fluid .mb-4}

You can see that there's hardly any class imbalance in the dataset. We have a total of 10,000 images, and each class appears roughly 1,000 times. This is perfect. 

#### Bonus: Plot The First Digit As ASCII Art

When I prompted my AI agent to load the dataset, it also helpfully generated code to display the first handwritten digit as ASCII art on the console. You can copy my code if you want, or you can prompt your own agent to write the code for you. 

Here's what I got. My agent created a new method called `VisualizeDigit` that looks like this:

```csharp
// Helper method to visualize a digit as ASCII art
static void VisualizeDigit(float[] pixels)
{
    for (int row = 0; row < 28; row++)
    {
        for (int col = 0; col < 28; col++)
        {
            int index = row * 28 + col;
            float pixelValue = pixels[index];
            
            // Convert pixel value to ASCII character
            char displayChar;
            if (pixelValue == 0)
                displayChar = ' ';
            else if (pixelValue < 64)
                displayChar = '.';
            else if (pixelValue < 128)
                displayChar = '+';
            else if (pixelValue < 192)
                displayChar = '*';
            else
                displayChar = '#';
            
            Console.Write(displayChar);
        }
        Console.WriteLine();
    }
}
```

This is cool! And in the main program method, it added the following code:

```csharp
// Visualize a sample digit (optional)
Console.WriteLine("\nVisualizing first digit (28x28 ASCII art):");
var firstRecord = dataList.First();
VisualizeDigit(firstRecord.PixelValues);
```

So when I run my app, I see this:

![First Digit As ASCII Art](../img/first-digit.png)
{.img-fluid .mb-4}

You can see that the image is clearly a handwritten number 7, which is the first image in the MNIST dataset.

{{< /encrypt >}}

# Design And Build The Transformation Pipeline

Now let's start designing the ML.NET data transformation pipeline. This is the sequence of feature engineering steps that will transform the dataset into something suitable for a machine learning algorithm to train on.

{{< encrypt >}}

#### Decide Feature Engineering Steps

Actually there's only one step we can do:

- Normalize the pixel values

Because nothing else is applicable here. There are no outliers, so we don't need to filter the data. There are no numerical features we can bin or categorical features we can one-hot encode. And there is no class imbalance, so there's no need for over- or undersampling. 

In fact, the only transformation we usually do for image datasets is scaling (and optionally converting to grayscale). We don't use any of the usual feature engineering steps. 

#### Implement The Transformation Pipeline

Let's ask Copilot to add a normalization step to the machine learning pipeline. Enter the following prompt in the Copilot panel:

"Add a normalization step to the machine learning pipeline to normalize the pixel values"
{ .prompt }

Let's take a look at the code. The pipeline will look like this:

```csharp
// Step 1: Convert Label to Key (categorical)
var pipeline = mlContext.Transforms.Conversion.MapValueToKey("Label")
    // Step 2: Normalize pixel values from 0-255 to 0-1 range
    .Append(mlContext.Transforms.NormalizeMinMax("PixelValues", "PixelValues"))
    // Step 3: Add a multiclass classifier (using SDCA)
    .Append(mlContext.MulticlassClassification.Trainers.SdcaMaximumEntropy("Label", "PixelValues"))
    // Step 4: Convert prediction back to original values
    .Append(mlContext.Transforms.Conversion.MapKeyToValue("PredictedLabel"));
```

This code uses `MapValueToKey` to convert the **Label** column to a key value that a learning algorithm can work with. Then the `NormalizeMinMax` method normalizes the **PixelValues** column and we select the `SdcaMaximumEntropy` algorithm to train the machine learning model. Finally, we need a closing `MapKeyToValue` step to convert any predicted keys back to their corresponding label values.

With multiclass classification, we always need pipelines that start with `MapValueToKey` and end with `MapKeyToValue`. This removes the need for one-hot encoding the label, because a 'key' in ML.NET is comparable to a one-hot encoded column.

Your AI agent may have used a different learning algorithm in your pipeline, but there's a good chance it picked SDCA too. LLMs know about many public machine learning datasets, and will usually pick the best learning algorithm for the job at hand. 

#### Split The Dataset

If your AI agent is smart, it will have also generated code to split the dataset, train a model, generate predictions for the test partition and evaluate the quality of the predictions. My Claude 4.0 agent did all that when I only asked for the normalization step. 

If you don't have the code to split your dataset yet, feel free to prompt your AI agent now: 

"Split the transformed data into two partitions: 80% for training and 20% for testing."
{ .prompt }

You should get the following code:

```csharp
// Split data into training and test sets
var trainTestSplit = mlContext.Data.TrainTestSplit(dataView, testFraction: 0.2);
var trainData = trainTestSplit.TrainSet;
var testData = trainTestSplit.TestSet;
```

The `TrainTestSplit` method splits a dataset into two parts, with the `testFraction` argument specifying how much data ends up in the second part.

#### Run The Pipeline And Generate Predictions

And finally, you'll see the following code to perform the transformations and get access to the transformed data:

```csharp
// Train the model
var model = pipeline.Fit(trainData);

// Make predictions on test set
var predictions = model.Transform(testData);
```

This code calls `Fit` to generate a machine learning model that implements the pipeline. The `Transform` method then uses this model to generate predictions for each image in the `testData` partition. 

Now we're ready to calculate the classification metrics. 

{{< /encrypt >}}

# Evaluate The Results

Now let's evaluate the quality of the model by comparing the predictions made on the 20% test data to the actual digits, and calculate the classification evaluation metrics.

So imagine you're scanning a stack of printed documents with lots of handwritten numbers. What level of accuracy would you consider acceptable?

{{< encrypt >}}

Determine the minimum accuracy you deem acceptable for OCR of handwritten digits. This will be the target your model needs to beat.
{ .homework }

#### Calculate Evaluation Metrics

Enter the following prompt:

"Use the trained model to create predictions for the test set, and then calculate evaluation metrics for these predictions and print them."
{ .prompt }

That should create the following code:

```csharp
// Make predictions on test set
var predictions = model.Transform(testData);

// Evaluate the model
var metrics = mlContext.MulticlassClassification.Evaluate(predictions, "Label", "Score");
Console.WriteLine($"\nModel Evaluation Results:");
Console.WriteLine($"Macro Accuracy: {metrics.MacroAccuracy:P2}");
Console.WriteLine($"Micro Accuracy: {metrics.MicroAccuracy:P2}");
Console.WriteLine($"Log-Loss: {metrics.LogLoss:F4}");
Console.WriteLine($"Log-Loss Reduction: {metrics.LogLossReduction:F4}");
```

This code calls `Transform` to set up predictions for every patient in the test partition. The `MulticlassClassification.Evaluate` method then compares these predictions to the actual diagnoses and automatically calculates these metrics:

- **MicroAccuracy**: this is the average accuracy (=the number of correct predictions divided by the total number of predictions) for every digit in the dataset.
- **MacroAccuracy**: this is calculated by first calculating the average accuracy for each unique prediction value, and then taking the averages of those averages.
- **LogLoss**: this is a metric that expresses the size of the error in the predictions the model is making. A logloss of zero means every prediction is correct, and the loss value rises as the model makes more and more mistakes.
- **LogLossReduction**: this metric is also called the Reduction in Information Gain (RIG). It expresses the probability that the model’s predictions are better than random chance.

We can compare the micro- and macro accuracy to discover if the dataset is biased. In an unbiased set each unique label value will appear roughly the same number of times, and the micro- and macro accuracy values will be close together. 

We already checked for any class imbalance in the MNIST dataset by generating a histogram of labels. The histogram looked fine, with roughly equal populations for each unique digit. So we expect the micro- and macro accuracy values to be close together. 

For the SDCA learning algorithm, you should get something like the following:

![Binary Classification Model Evaluation](../img/evaluate.png)
{ .img-fluid .mb-4 }

Let's analyze my results:

The Macro Accuracy is **91.13%**. This is the average accuracy computed independently for each class, then averaged. It treats all classes equally, even if some digits appear more often than others. This metric is good for judging balanced performance across the entire dataset. 

A value of 91.13% is good. The model is performing well across all digits, with no single class dragging the average down too much.

The Micro Accuracy is **91.34%**. This is the overall accuracy across all predictions, weighting each instance equally. It basically means the "percentage of correct predictions" on the entire dataset, and will be affected by bias if the dataset is class-imbalanced.

A value of 91.34% is good. Over 9 out of 10 predictions are correct, which is solid for a model trained on 10k images. Also note that the micro- and macro accuracies are close together, which means that the dataset does not have a class-imbalance. This is what we expected, as we already checked for class imbalance earlier.

The Log-Loss is **0.3538**. This measures how well the model’s predicted probabilities match the true labels. Lower is better: 0 means perfect confidence and correctness, and higher values mean more wrong or overconfident predictions.

A value of 0.3538 is good. This is fairly low, meaning the probability outputs are reasonably well-calibrated and confident when correct.

Ths Log-Loss Reduction is **0.8460**. This shows how much better the model’s log-loss is compared to a naïve baseline (e.g., guessing by class frequency). A value close to 1 means a big improvement over random guessing.

A value of 0.8460 is very good. The model shows an 84.6% improvement over baseline which shows that it performs far better than random guessing.

So how did your model do?

Compare your model with the target you set earlier. Did it make predictions that beat the target? Are you happy with the predictive quality of your model? Can you explain what each metric means for the quality of your predictions? 
{ .homework }

#### Generate The Confusion Matrix

Printing the confusion matrix is super easy because ML.NET has a built-in method that does everything for us. We don't have to prompt our AI agent for this, because you can do it in a single line of code:

```csharp
// Display confusion matrix
Console.WriteLine(metrics.ConfusionMatrix.GetFormattedConfusionTable());
```

The multiclass evaluation metrics object has a property `ConfusionMatrix` to access the matrix, and the `GetFormattedConfusionTable` method returns the full table as a string that we can write directly to the console. 

Homework: Add this code to your app, then run the app and examine the confusion matrix. Is this what you expected? Write down your observations.  
{ .homework }

My matrix looks like this:

![Confusion Matrix](../img/confusion-print.png)
{ .img-fluid .mb-4 }

In the matrix, each column holds a prediction and each row corresponds to a ground truth label value. The main diagonal shows all correct predictions, and all the errors are off the diagonal. We also get precison and recall values for each individual class.

But there is a problem with this matrix. What you're seeing on the x- and y-axis are not the class labels, but the class label indices. Remember that `MapValueToKey` step in the machine learning pipeline? It converted every unique class label to a numeric index (the 'Key') and that's what we're seeing in the matrix right now. 

So if you look closely at the image, you can see 13 errors made by the model when predicting the class number 5 which was actually class number 4. But these two classes could be any value, and are not guaranteed to match the digits 5 and 4. 

To generate a confusion matrix with sensible axis labels, we'll need to add some extra code. We'll do that next.

#### Plot The Confusion Matrix

Now let's plot this matrix with ScottPlot, just like we did with the Cleveland CAD dataset. Enter the following prompt:

"Add code to plot the confusion matrix as a heatmap with Scottplot. Use the ConfusionMatrix.GetCountForClassPair method provided by ML.NET"
{ .prompt }

Note that I'm asking for a calculation that uses `GetCountForClassPair`, a built-in ML.NET method that calculates the prediction count for any cell in the matrix. This will (hopefully) prevent the agent from doing the calculation on its own. 

Carefully examine the generated code, for it's very easy for AI agents to make mistakes here. What you want to see is the following code:

```csharp
// Build an array to map classes to key indices
var labelColumn = predictions.Schema["Label"];
var keyValues = new VBuffer<float>(); 
labelColumn.GetKeyValues(ref keyValues);
float[] classLabels = keyValues.DenseValues().ToArray();
```

This code accesses the schema of the **Label** column and uses `GetKeyValues` to obtain the list of key values (stored as a `VBuffer`). Then a call to `DenseValues` converts the vbuffer to a float array.

We now have a `classLabels` array with label values, with array indices that exactly match the key indices the machine learning pipeline is using. 

When AI agents work on multiclass dataset with the ML.NET library, they often confuse class label values and key indices. Be extra attentive and double-check your code. Every call to GetCountForClassPair should provide indices, not label values. 
{ .tip }

With that, we can assemble a confusion matrix like this:

```csharp
// Get the confusion matrix
var confusionMatrix = metrics.ConfusionMatrix;

// Get the number of classes
int numClasses = confusionMatrix.NumberOfClasses;

// Create a 2D array for the confusion matrix data
double[,] matrixData = new double[numClasses, numClasses];

// Fill the matrix using GetCountForClassPair
for (int actualClass = 0; actualClass < numClasses; actualClass++)
{
    for (int predictedClass = 0; predictedClass < numClasses; predictedClass++)
    {
        var predictionIndex = Array.IndexOf(classLabels, predictedClass);
        var actualIndex = Array.IndexOf(classLabels, actualClass);
        var count = confusionMatrix.GetCountForClassPair(predictionIndex, actualIndex);
        matrixData[actualClass, predictedClass] = count;
    }
}
```

The code loops over each possible class pair and uses `Array.IndexOf` to convert the class values to their corresponding key indices. The `GetCountForClassPair` method uses the indices to get the number of predictions for that class pair. It then builds a 2-dimensional matrix with each pair in the correct position. 

The plotting code that follows is similar to what we used to generate the Confusion matrix heatmap for the Cleveland CAD dataset. 

Homework: Run your app and examine the plotted confusion matrix. Examine the most popular incorrect predictions. Do they make sense? Write down your observations. 
{ .homework }

My matrix looks like this:

![Confusion Matrix](../img/confusion-matrix.png)
{ .img-fluid .mb-4 }

The main diagonal contains all correct predictions, and each cell is deep black as expected. The matrix cells off the main diagonal describe incorrect predictions, and from the plot we can quickly see that the most popular mistake is when the model predicts a '9' but the digit is actually a '4'. A second common mistake is when the model predicts a '3' but the digit is actually a '5'. 

These mistakes make sense if we consider the quality of the handwriting in the MNIST dataset. I mean, look at this:

![Confusion Matrix](../img/mnist-examples.png)
{ .img-fluid .mb-4 }

These are a couple of random digits from the dataset. You can see several digits in this example that could either be a '4' or a '9', so the machine learning model does not have an easy task sorting out which is which. 

#### Create A Utility Class

Let's put the code for plotting the confusion matrix into a utility class so that we can reuse the code in later lab modules and lessons.

In Visual Studio Code, select the code that generates the confusion matrix plot. Then press CTRL+I to launch the in-line AI prompt window, and type the following prompt:

"Move all of this code to a new method PlotConfusionMatrix, and put this method in a new utility class called MulticlassUtils."
{ .prompt }

This cleaned up my main method a lot and only left the following code:

```csharp
// Plot confusion matrix as heatmap
var plot = MulticlassUtils.PlotConfusionMatrix(metrics.ConfusionMatrix, classLabels);
plot.SavePng("confusion-matrix.png", 900, 600);
```

If you get stuck or want to save some time, feel free to download my completed MulticlassUtils class from Codeberg and use it in your own project:

https://codeberg.org/mdft/ml-mlnet-csharp/src/branch/main/Mnist/MulticlassUtils.cs


#### Next Steps

Next, let's add a prediction engine to the machine learning app to make a few ad-hoc predictions for random digits in the dataset.

{{< /encrypt >}}

# Test A Prediction

To wrap up, let’s use the model to test a prediction.

We are going to identify the class pair (predicted versus actual label) that the model struggles with the most, select one sample image from that pair, run a prediction and then show the image as ASCII art. That should give us a clue why the model struggles with that particular pair. 

Let's see if our AI agent can write all code in one go. Enter the following prompt:

{{< encrypt >}}

"Add code to identify the class pair (predicted versus actual label) that the model struggles with the most, select one sample record from that pair, then use the model to generate a prediction for that digit, and finally show the digit as ASCII art for comparison." 
{ .prompt }

#### Find The Most Popular Incorrect Prediction

The agent will add code like this to identify the class pair corresponding to the most popular incorrect prediction:

```csharp
// Get the number of classes
var confusionMatrix = metrics.ConfusionMatrix;
int numClasses = confusionMatrix.NumberOfClasses;

// Find the highest off-diagonal value (most confused pair)
int maxConfusedActual = -1;
int maxConfusedPredicted = -1;
double maxConfusionCount = 0;

for (int actual = 0; actual < numClasses; actual++)
{
    for (int predicted = 0; predicted < numClasses; predicted++)
    {
        if (actual != predicted) // Skip diagonal (correct predictions)
        {
            var predictionIndex = Array.IndexOf(classes, predicted);
            var actualIndex = Array.IndexOf(classes, actual);
            double count = confusionMatrix.GetCountForClassPair(predictionIndex, actualIndex);
            if (count > maxConfusionCount)
            {
                maxConfusionCount = count;
                maxConfusedActual = actual;
                maxConfusedPredicted = predicted;
            }
        }
    }
}
```

This is the same code as before. It uses `Array.IndexOf` to calculate the indices of every class pair, and `GetCountForClassPair` to find the number of predictions for that class pair. The pair for the worst-performing incorrect predictions gets stored in `maxConfusedActual` and `maxConfusedPredicted`.

#### Sample One Incorrect Prediction

Next, we have to select one sample prediction for the worst performing class pair. My agent came up with this, and you'll probably have something similar in your app:

```csharp
// Get predictions with actual labels for analysis
var predictionList = mlContext.Data.CreateEnumerable<MnistPrediction>(predictions, reuseRowObject: false).ToList();
var actualList = mlContext.Data.CreateEnumerable<MnistData>(testData, reuseRowObject: false).ToList();

// Find the first sample from the most confused pair
int sample = -1;
for (int i = 0; i < predictionList.Count; i++)
{
    var prediction = predictionList[i].PredictedLabel;
    var actual = actualList[i].Label;
    if (actual == maxConfusedActual && prediction == maxConfusedPredicted)
    {
        sample = i;
        break;
    }
}
```

This code calls `CreateEnumerable` to create lists of predicted and actual labels, and then searches the lists for a pair that matches the worst performing class pair identified earlier. The code simply grabs the first prediction it can find that matches the pair. 

#### Print The Results

Finally, we can report the outcome like this:

```csharp
Console.WriteLine($"\nFirst image that matches class pair");
Console.WriteLine($"Row ID: {actualList[sample].RowID}");
Console.WriteLine($"Actual Label: {actualList[sample].Label}");
Console.WriteLine($"Predicted Label: {predictionList[sample].PredictedLabel}");

// Show confidence scores for all classes
Console.WriteLine("\nConfidence scores for all classes:");
for (int i = 0; i < predictionList[sample].Score.Length; i++)
{
    var index = Array.IndexOf(classes, i);
    Console.WriteLine($"Class {i}: {predictionList[sample].Score[index]:P2}");
}

// Display the digit as ASCII art
Console.WriteLine($"\nASCII Art Visualization of Digit:");
VisualizeDigit(actualList[sample].PixelValues);
```

This code is quite nice, my AI agent went above and beyond to report everything noteworthy about the sample. I get the actual and predicted label, the confidence scores for all 10 classes and the digit drawn as ASCII art.

Note that the `Score` property with the array of confidence scores is also indexed by key index, so we need another `Array.IndexOf` call to match each score with the correct class value. 

Homework: Inspect your code and make sure you handle class values and key indices correctly everywhere. Then run your app and examine the output. Look at the actual and predicted label, the prediction scores and the ASCII art. Does the incorrect prediction make sense to you? 
{ .homework }

My output looks like this:

![Confusion Matrix](../img/prediction-1.png)
{ .img-fluid .mb-4 }

We already knew that the most popular incorrect prediction is where the model predicts a 9 but the digit was actually a 4. The code samples image 160 which has this exact same class pair. We can see the individual confidence values, with **55.23%** confidence for a '9' and **44.19%** confidence for a '4'. Those values are pretty close together, so the model simply cannot decide between the one or the other digit. 

This is what image 160 looks like:

![Confusion Matrix](../img/prediction-2.png)
{ .img-fluid .mb-4 }

To me this clearly looks like a '4', but I guess the horizontal stroke is too short to make this digit clearly look like the number four. And there's definitely enough sloppy handwriting in the MNIST dataset to confuse the model. 

{{< /encrypt >}}

# Load The Full Dataset

So far, we have been working with a subset of the MNIST dataset. This subset contains only 10,000 rows, but the official MNIST dataset actually contains 60,000 images for training and 10,000 images for testing. 

So let's download the full dataset and see how our app holds up. 

{{< encrypt >}}

This [Github repository](https://github.com/phoebetronic/mnist) contains the full dataset files. Download the **mnist_train.zip** and **mnist_test.zip** files and unzip them to your project folder.

#### Load The Full Dataset

Now let's alter the app so that we load all 70,000 images. Enter the next prompt to refactor the application:

"Add code that alters the app so that it load the mnist_train.csv and mnist_test.csv files. Remove the RowID column (because it's not in these files) and adjust the LoadColumn attributes accordingly. Do not split the data, just use mnist_train to train the model and mnist_test to test the model. Fix all compiler errors due to this change."
{ .prompt }

This is a straightforward refactor, but the AI agent has to jiggle some variables around to make everything work. 

Homework: refactor your app to load the full dataset. Then run the app and examine the new evaluation metrics. What do you notice? Write down your observations.
{ .homework }

#### Evaluation Results

Here's what I got:

![Training a Model on all Images](../img/evaluate-full-1.png)
{.img-fluid .mb-4}

The macro accuracy is **91.0%**, slightly down from 91.13% for the partial dataset. This is still a great result. The model is performing well across all digits, with no single class dragging the average down too much.

The micro accuracy is **91.09%**, slightly down from 91.34% for the partial dataset. This is also great. Over 9 out of 10 predictions are correct, which is great for a model trained on 60,000 images. Also note that the micro- and macro accuracies are still close together, which means that the full dataset does not have a class-imbalance, just like the partial dataset.

The log loss is **0.3007**, slightly lower than 0.3538 for the partial dataset. Lower is better, so the probability outputs are now slightly better calibrated and confident when correct.

The log loss reduction is **0.8693**, slightly higher than 0.8460 for the partial dataset. Higher is better, so the model now shows an 86.93% improvement over baseline.

Remember the New York TLC dataset, where the results got worse when we loaded the full dataset? Well, here the opposite happened: we got slightly better results training on the full 60,000 images. It means that even with only 10,000 images, the model picked up enough patterns to be able to recognize every digit. The additional 50,000 training images were just more of the same, so the model hardly improved (or regressed).

Here's the next part of the output:

![Training a Model on all Images](../img/evaluate-full-2.png)
{.img-fluid .mb-4}

The most common incorrect prediction is still a '9' where the actual image turned out to be a '4'. This now happens 58 times. The sampled image has almost equal scores for 4 and 9: respectively **29.0%** and **29.23%**. The '9' prediction wins out by online a tiny margin. 

The next two scores, in descending order, are '8' (15.56%) and '6' (14.10%). 

And these scores make perfect sense when you look at the actual image:

![Training a Model on all Images](../img/evaluate-full-3.png)
{.img-fluid .mb-4}

That could definitely be a '4', or a '9'.

#### The Confusion Matrix

Here is the confusion matrix I got when training on the full MNIST dataset:

![The Confusion Matrix](../img/confusion-full.png)
{.img-fluid .mb-4}

The main diagonal contains all correct predictions, and each cell is deep black as expected. The matrix cells off the main diagonal describe incorrect predictions, and from the plot we can quickly see that the most popular mistakes are:

- The model predicts a '9' but the digit is a '4' (58 times)
- The model model predicts a '5' but the digit is an '8' (46 times)
- The model predicts an '8' but the digit is actually a '2' (46 times)
- The model predicts a '5' but the digit is actually a '3' (45 times)
- The model predicts a '9' but the digit is actually a '7' (44 times)

We've seen these mistakes before. There are a bit more of them now, but the total number of incorrect predictions is still quite small compared to the number of correct predictions (visible as the color contrast between the main diagonal and the other matrix cells).

All in all this is a pretty good result. We trained on the full dataset and everything is fine!

{{< /encrypt >}}

# Improve Your Results

There are many factors that influence the accuracy of your model. These include how you preprocess the images, which classification algorithm you choose, and how you configure the training hyperparameters.

Here are some strategies you can try to improve your model's performance:

{{< encrypt >}}

- Apply image augmentation techniques such as rotation, scaling, or shifting to increase the size of the dataset and give the model more data to train on.
- Try different learning algorithms.
- Tune hyperparameters for your chosen learning algorithm.
- Train individual models on each digit, then combine the predictions.
- Train a model to recognize each digit, then train a second model to identify and fix the incorrect predictions.

Experiment with the skills you learned so far. Try to improve your machine learning pipeline and document your best classification metrics for the MNIST dataset.
{ .homework }

How accurate can you make your digit recognition model?

{{< /encrypt >}}

# Hall Of Fame

Would you like to be famous? You can [submit your best-performing model](mailto:mark@mdfteurope.com) for inclusion in this hall of fame, which lists the best multiclass classification evaluation scores for the MNIST dataset. I've added my own results as a baseline, using the transformations I mentioned in the lab. 

Can you beat my score?

| Rank | Name | Algorithm      | Transformations | Micro Accuracy | Macro Accuracy |
|------|------|----------------|-----------------|----------------|----------------|
|  1   | Mark | SDCA Max Entropy | As mentioned in lab | 0.9109 | 0.91 |

I will periodically collect new submissions and merge them into the hall of fame. I'll share the list in my courses and on social media. If you make the list, you'll be famous!

# Recap

Congratulations on finishing the lab. Here's what you have learned.

{{< encrypt >}}

You learned how to analyze the **MNIST dataset**, both automatically by having an AI agent scan the data for you, and manually by inspecting the data by hand. You then loaded the dataset into a machine learning pipeline, and learned that you can use the **VectorType** attribute to load all the pixel features into a single array of floats. 

You did not generate the histograms of features or the Pearson correlation matrix, because these tools are not very useful when working with image data. Instead, you generated a **histogram of labels** to check if the dataset is **class-imbalanced**.

You built a machine learning pipeline consisting of **value-to-key mappings** and a normalization step. You learned that ML.NET can predict multiclass labels of any type by converting them to keys. Then you trained a multiclass classification model on the data and evaluated the quality of the predictions with the **micro accuracy** and **macro accuracy** metrics.

When generating the confusion matrix, you discovered that ML.NET uses **key index values** in many properties and methods, which need to be translated back to their corresponding label values. 

You analyzed the classification metrics and the confusion matrix, both for the truncated dataset with only 10,000 imaged an the full dataset of 70,000 images. You discoverded that the prediction quality improved when training on the full dataset.

You completed the lab by experimenting with different data processing steps and classification algorithms to find the best-performing model. 

{{< /encrypt >}}

# Conclusion

This concludes the lab on multiclass classification.

This was your first Computer Vision project, where you used a machine learning model to classify a collection of images. Computer vision is conceptually really simple, all you're doing is feeding every pixel of an image into a machine learning model as a number.

Your pipeline used a classification algorithm derived from logistic regression. But if you really want to get superhuman results, you'll need a much better algorithm: the convolutional deep neural network (CNN).

A CNN slides a small pattern-matching engine called a convolution matrix across an image to detect simple patterns like edges, shapes, and textures. Deeper layers inside the neural network are able to pick up more complex patterns like fur, clouds, a sausage, and so on.

For the last 10 years or so, fully trained CNNs have been able to achieve an accuracy of 99.9% on standardized image recognition tasks, which is far above the average human performance level. This is a domain where AI has achieved superhuman performance.

This is why research into self-driving cars and mobile autonomous robots has increased significantly in the last decade. Computer vision is now a solved problem in AI, so we're now very close to having robots that can autonomously navigate their environment. 
